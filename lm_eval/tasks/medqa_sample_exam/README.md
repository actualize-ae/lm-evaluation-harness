# MedQA Sample Exam

### Paper

Title: `MedQA Sample Exam: A Large-scale Open Domain Question Answering Dataset from Medical Exams`

Abstract: `https://arxiv.org/abs/2009.13081`

`What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams, 
This contains the English portion of the full MedQA dataset, containing 12,723 multiple (4) choice questions from the US medical licensing exam.`

Homepage: `https://paperswithcode.com/dataset/medqa-usmle`


### Citation

```
@misc{jin2020disease,
    title={What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams}, 
    author={Di Jin and Eileen Pan and Nassim Oufattole and Wei-Hung Weng and Hanyi Fang and Peter Szolovits},
    year={2020},
    eprint={2009.13081},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
```

### Groups and Tasks

#### Groups

* `group_name`: `Short description`

#### Tasks

* `MedQA Sample Exam`: `A Large-scale Open Domain Question Answering Dataset from Medical Exams`

### Checklist

For adding novel benchmarks/datasets to the library:
* [ ] Is the task an existing benchmark in the literature?
  * [ ] Have you referenced the original paper that introduced the task?
  * [ ] If yes, does the original paper provide a reference implementation? If so, have you checked against the reference implementation and documented how to run such a test?


If other tasks on this dataset are already supported:
* [ ] Is the "Main" variant of this task clearly denoted?
* [ ] Have you provided a short sentence in a README on what each new variant adds / evaluates?
* [ ] Have you noted which, if any, published evaluation setups are matched by this variant?
